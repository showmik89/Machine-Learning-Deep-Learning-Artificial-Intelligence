# -*- coding: utf-8 -*-
"""Enhanced Dermatological Disease Classification through DenseNet Architectures A Deep Learning Approach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1723yWxujWosSVJ1dMmsRUK_OpzixZgs2
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
print(tf.__version__)

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
fid = drive.ListFile({'q':"title='israt.zip'"}).GetList()[0]['id']
f = drive.CreateFile({'id': fid})
f.GetContentFile('israt.zip')
f.keys()
!unzip israt.zip

train_dir = '/content/israt/Training'
validation_dir = '/content/israt/Validation'
test_dir = '/content/israt/Test'

BATCH_SIZE = 128
IMG_SIZE = (224, 224)
no_classes = 5

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

class_names = train_dataset.class_names
print(class_names)

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

#Use buffered prefetching to load images from disk without having I/O become blocking.
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

data_augmentation = tf.keras.Sequential([
  #tf.keras.layers.Resizing(224,224),
  #tf.keras.layers.CenterCrop(224,224),
  tf.keras.layers.RandomFlip('horizontal_and_vertical'),
  tf.keras.layers.RandomRotation(0.2),
  #tf.keras.layers.RandomZoom(height_factor=(0.1, 0.2), width_factor=(0.1, 0.2)),
  #tf.keras.layers.RandomContrast(0.2),
  #RandomBrightness(0.3),
  #tf.keras.layers.RandomBrightness(0.5)
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0].numpy().astype("uint8"))
    plt.axis('off')

IMG_SHAPE = IMG_SIZE + (3,)
print(IMG_SHAPE)
base_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE, weights="imagenet", include_top=False)
image_batch, label_batch = next(iter(train_dataset))
print(image_batch.shape)
feature_batch = base_model(image_batch)
print(feature_batch.shape)

print("Number of layers in the base model: ", len(base_model.layers))

#Freeze the convolutional base
base_model.trainable = False

# Let's take a look at the base model architecture
base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(no_classes, activation='softmax')
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

inputs = tf.keras.Input(shape=IMG_SHAPE)
x = data_augmentation(inputs)
x = tf.keras.applications.densenet.preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.5)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()

class SaveConvertCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(SaveConvertCallback, self).__init__()

    def on_train_end(self, logs=None):
        self.model.save('model')
        self.model.save('model.h5')

EarlyStop = tf.keras.callbacks.EarlyStopping(patience=2)

history = model.fit(train_dataset,
                    epochs=200,
                    validation_data=validation_dataset,
                    callbacks=[EarlyStop, SaveConvertCallback()])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

from tensorflow.python.ops.gen_logging_ops import Print
#loss, accuracy = model.evaluate(validation_dataset)
#print('Test accuracy :', accuracy)
target_names = ['Atopic Dermatitis(0)', 'Herpes(1)', 'Lyme disease(2)', 'Poison Ivy(3)', 'Psoriasis(4)']
predictions = np.array([])
labels =  np.array([])
for x, y in test_dataset:
  z = model.predict(x, verbose = 0)
  #print(z[0])
  predictions = np.concatenate([predictions, np.argmax(z, axis=-1)])
  #print(predictions)
  labels = np.concatenate([labels, y.numpy()])
  #print(labels)

print(tf.math.confusion_matrix(labels=labels, predictions=predictions).numpy())

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print(classification_report(y_true=labels, y_pred=predictions, target_names=target_names))

cm = confusion_matrix(y_true=labels, y_pred=predictions)
print(cm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['AD', 'H', 'LD', 'PI', 'P'])
disp.plot(cmap=plt.cm.Blues_r,values_format='g')
plt.show()

losst, accuracyt = model.evaluate(test_dataset, verbose = 0)
print('Test accuracy :', accuracyt)

#Fine-tuning start here
base_model.trainable = True
# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model.layers))

# Fine-tune from this layer onwards
fine_tune_at = 377

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()
len(model.trainable_variables)

initial_epochs = 136
fine_tune_epochs = 100
total_epochs =  initial_epochs + fine_tune_epochs

class SaveConvertCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(SaveConvertCallback, self).__init__()

    def on_train_end(self, logs=None):
        self.model.save('model')
        self.model.save('model.h5')

EarlyStop = tf.keras.callbacks.EarlyStopping(patience=2)

history_fine = model.fit(train_dataset,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=validation_dataset,
                         callbacks=[EarlyStop, SaveConvertCallback()])

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss = history.history['loss']
loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

import matplotlib.pyplot as plt

from tensorflow.python.ops.gen_logging_ops import Print
#loss, accuracy = model.evaluate(validation_dataset)
#print('Test accuracy :', accuracy)
target_names = ['Atopic Dermatitis(0)', 'Herpes(1)', 'Lyme disease(2)', 'Poison Ivy(3)', 'Psoriasis(4)']
predictions = np.array([])
labels =  np.array([])
for x, y in test_dataset:
  z = model.predict(x, verbose = 0)
  predictions = np.concatenate([predictions, np.argmax(z, axis=-1)])
  #print(predictions)
  labels = np.concatenate([labels, y.numpy()])
  #print(labels)

print(tf.math.confusion_matrix(labels=labels, predictions=predictions, num_classes = 11).numpy())

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_true=labels, y_pred=predictions)
#print(cm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['AD', 'H', 'LD', 'PI', 'P'])
disp.plot(cmap=plt.cm.Blues_r,values_format='g')
plt.show()

print(classification_report(y_true=labels, y_pred=predictions, target_names=target_names))

loss, accuracy = model.evaluate(test_dataset, verbose = 0)
print('Test accuracy :', accuracy)

